{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e70ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial numeric predictors: ['log_seconds_after_rat_arrival', 'hours_after_sunset_sq', 'risk_reward_interaction', 'efficiency_ratio', 'interaction_rat_risk', 'reward_to_risk_ratio', 'nocturnal_index']\n",
      "Dropping 'hours_after_sunset_sq' due to high VIF (inf)\n",
      "Dropping 'log_seconds_after_rat_arrival' due to high VIF (10.58)\n",
      "\n",
      "Final predictors after VIF filtering:\n",
      "['risk_reward_interaction', 'efficiency_ratio', 'interaction_rat_risk', 'reward_to_risk_ratio', 'nocturnal_index']\n",
      "                             OLS Regression Results                            \n",
      "===============================================================================\n",
      "Dep. Variable:     bat_landing_to_food   R-squared:                       0.153\n",
      "Model:                             OLS   Adj. R-squared:                  0.149\n",
      "Method:                  Least Squares   F-statistic:                     32.62\n",
      "Date:                 Wed, 15 Oct 2025   Prob (F-statistic):           1.23e-30\n",
      "Time:                         02:14:17   Log-Likelihood:                -4221.7\n",
      "No. Observations:                  907   AIC:                             8455.\n",
      "Df Residuals:                      901   BIC:                             8484.\n",
      "Df Model:                            5                                         \n",
      "Covariance Type:             nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      14.8789      4.154      3.582      0.000       6.727      23.031\n",
      "risk_reward_interaction    31.0652      3.352      9.267      0.000      24.486      37.644\n",
      "efficiency_ratio          -25.9606      4.224     -6.146      0.000     -34.250     -17.671\n",
      "interaction_rat_risk       -5.5208      3.304     -1.671      0.095     -12.004       0.963\n",
      "reward_to_risk_ratio       -1.1785      3.627     -0.325      0.745      -8.297       5.940\n",
      "nocturnal_index             1.4692      3.711      0.396      0.692      -5.813       8.752\n",
      "==============================================================================\n",
      "Omnibus:                     1269.155   Durbin-Watson:                   1.948\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           311264.701\n",
      "Skew:                           7.639   Prob(JB):                         0.00\n",
      "Kurtosis:                      92.459   Cond. No.                         10.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "0.16995033004685722\n",
      "\n",
      "Evaluation Metrics:\n",
      "Test R²:   0.170\n",
      "Test RMSE: 19.367\n",
      "Test MAE:  10.670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maze/Documents/FODS/env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV R²: 0.080 (± 0.163)\n",
      "\n",
      "All outputs saved in: /Users/maze/Documents/FODS/outputs\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Investigation A — Revisited (Linear Regression with Feature Engineering)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load datasets\n",
    "# --------------------------------------------------\n",
    "df1 = pd.read_csv(\"datasets/dataset1.csv\")\n",
    "df2 = pd.read_csv(\"datasets/dataset2.csv\")\n",
    "\n",
    "# Optional: merge datasets if a key exists (you can edit this line if you know the key)\n",
    "# For now, we use df1 for Investigation A\n",
    "#df = pd.concat([df1, df2], ignore_index=True)\n",
    "df = df1.copy()\n",
    "\n",
    "# Output directory\n",
    "OUT = Path(\"outputs\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Select Response Variable\n",
    "# --------------------------------------------------\n",
    "# Choose the main behaviour-related continuous variable\n",
    "response = \"bat_landing_to_food\"\n",
    "assert response in df.columns, f\"{response} not found in dataset.\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Feature Engineering\n",
    "# --------------------------------------------------\n",
    "# Drop rows with missing response\n",
    "df = df.dropna(subset=[response]).copy()\n",
    "\n",
    "df['rat_present'] = np.nan\n",
    "if 'seconds_after_rat_arrival' in df1.columns:\n",
    "    df1.loc[df1['seconds_after_rat_arrival'].notna(), 'rat_present'] = (df1.loc[df1['seconds_after_rat_arrival'].notna(), 'seconds_after_rat_arrival'] >= 0).astype(int)\n",
    "if set(['start_time','rat_period_start','rat_period_end']).issubset(df1.columns):\n",
    "    mask = df1['rat_present'].isna() & df1['start_time'].notna() & df1['rat_period_start'].notna() & df1['rat_period_end'].notna()\n",
    "    df1.loc[mask, 'rat_present'] = ((df1.loc[mask,'start_time'] >= df1.loc[mask,'rat_period_start']) & (df1.loc[mask,'start_time'] <= df1.loc[mask,'rat_period_end'])).astype(int)\n",
    "df['rat_present'] = df1['rat_present'].fillna(0).astype(int)\n",
    "\n",
    "# Basic variable transformations\n",
    "if 'hours_after_sunset' in df.columns:\n",
    "    df['hours_after_sunset_sq'] = df['hours_after_sunset']**2\n",
    "\n",
    "if 'seconds_after_rat_arrival' in df.columns and 'hours_after_sunset' in df.columns:\n",
    "    df['interaction_sec_x_hours'] = df['seconds_after_rat_arrival'] * df['hours_after_sunset']\n",
    "df['risk_reward_interaction'] = df['risk'] * df['reward']\n",
    "\n",
    "\n",
    "df['efficiency_ratio'] = df['reward'] / (df['bat_landing_to_food'] + 1)\n",
    "df['log_seconds_after_rat_arrival'] = np.log1p(df['seconds_after_rat_arrival'])\n",
    "# Convert categorical columns\n",
    "\n",
    "\n",
    "df['hours_after_sunset_sq'] = df['hours_after_sunset'] ** 2\n",
    "df['log_seconds'] = np.log1p(df['seconds_after_rat_arrival'])\n",
    "df['interaction_rat_risk'] = df['rat_present'] * df['risk']\n",
    "df['interaction_time_risk'] = df['hours_after_sunset'] * df['risk']\n",
    "df['rat_time_effect'] = df['rat_present'] * df['seconds_after_rat_arrival']\n",
    "df['reward_to_risk_ratio'] = df['reward'] / (df['risk'] + 1)\n",
    "df['nocturnal_index'] = np.sin((df['hours_after_sunset'] / 24) * 2 * np.pi)\n",
    "\n",
    "\n",
    "# Log-transform skewed numeric variables\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if (df[col] > 0).all():  # log1p safe only for positive\n",
    "        if abs(df[col].skew()) > 1:\n",
    "            df[f'{col}_log1p'] = np.log1p(df[col])\n",
    "\n",
    "# Polynomial features (non-linear transformation)\n",
    "poly_features = []\n",
    "if 'hours_after_sunset' in df.columns:\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    poly_df = pd.DataFrame(poly.fit_transform(df[['hours_after_sunset']]),\n",
    "                           columns=['hours_after_sunset', 'hours_after_sunset_sq'])\n",
    "    df = pd.concat([df, poly_df], axis=1)\n",
    "    poly_features = poly_df.columns.tolist()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Predictor Selection\n",
    "# --------------------------------------------------\n",
    "predictors = [\"log_seconds_after_rat_arrival\",\"hours_after_sunset_sq\",\"risk_reward_interaction\", \"efficiency_ratio\", 'interaction_rat_risk','reward_to_risk_ratio','nocturnal_index'  ]\n",
    "print(\"Initial numeric predictors:\", predictors)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Multicollinearity Check and Reduction (VIF)\n",
    "# --------------------------------------------------\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Initial model predictors\n",
    "X = df[predictors].select_dtypes(include=[np.number]).copy().fillna(0)\n",
    "y = df[response]\n",
    "\n",
    "# Remove high-VIF features (>10)\n",
    "vif = calculate_vif(X)\n",
    "while vif[\"VIF\"].max() > 10:\n",
    "    drop_col = vif.sort_values(\"VIF\", ascending=False)[\"feature\"].iloc[0]\n",
    "    print(f\"Dropping '{drop_col}' due to high VIF ({vif['VIF'].max():.2f})\")\n",
    "    X = X.drop(columns=[drop_col])\n",
    "    vif = calculate_vif(X)\n",
    "\n",
    "print(\"\\nFinal predictors after VIF filtering:\")\n",
    "print(list(X.columns))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Fit Linear Regression (OLS)\n",
    "# --------------------------------------------------\n",
    "X_const = sm.add_constant(X)\n",
    "ols_model = sm.OLS(y, X_const).fit()\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Save model summary\n",
    "with open(OUT / \"InvestigationA_OLS_summary.txt\", \"w\") as f:\n",
    "    f.write(ols_model.summary().as_text())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Diagnostics\n",
    "# --------------------------------------------------\n",
    "residuals = ols_model.resid\n",
    "fitted = ols_model.fittedvalues\n",
    "\n",
    "# Residuals vs Fitted\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=fitted, y=residuals, alpha=0.6)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title(\"Investigation A: Residuals vs Fitted\")\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"A_resid_vs_fitted.png\")\n",
    "plt.close()\n",
    "\n",
    "# QQ plot\n",
    "sm.qqplot(residuals, line='45')\n",
    "plt.title(\"Investigation A: QQ Plot\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"A_qq.png\")\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. Train-Test Evaluation\n",
    "# --------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(r2)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Test R²:   {r2:.3f}\")\n",
    "print(f\"Test RMSE: {rmse:.3f}\")\n",
    "print(f\"Test MAE:  {mae:.3f}\")\n",
    "\n",
    "# Cross-validation (5-fold)\n",
    "cv_scores = cross_val_score(lr, X, y, cv=5, scoring='r2')\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.3f} (± {cv_scores.std():.3f})\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9. Save Cleaned Data & Outputs\n",
    "# --------------------------------------------------\n",
    "df.to_csv(OUT / \"InvestigationA_cleaned.csv\", index=False)\n",
    "vif.to_csv(OUT / \"InvestigationA_VIF.csv\", index=False)\n",
    "print(\"\\nAll outputs saved in:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4610452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b00fcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['temperature_scaled', 'humidity_scaled']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/m0/6cggrtcs02gfymtzntgnsn700000gn/T/ipykernel_97551/3306237283.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[33m'season_sin'\u001b[39m, \u001b[33m'season_cos'\u001b[39m, \u001b[33m'night_intensity'\u001b[39m\n\u001b[32m     44\u001b[39m ]\n\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Drop missing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m dfA = dfA.dropna(subset=[response] + predictorsA)\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- Remove multicollinearity ---\u001b[39;00m\n\u001b[32m     50\u001b[39m X = dfA[predictorsA].assign(Intercept=\u001b[32m1\u001b[39m)\n",
      "\u001b[32m~/Documents/FODS/env/lib/python3.12/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6673\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6674\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6675\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6676\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6677\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6678\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6679\u001b[39m \n\u001b[32m   6680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['temperature_scaled', 'humidity_scaled']"
     ]
    }
   ],
   "source": [
    "# Investigation A: Enhanced Linear Regression Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "dfA = df1.copy()\n",
    "\n",
    "# Log-transform (avoid zero/negatives)\n",
    "dfA['log_seconds_after_rat_arrival'] = np.log1p(dfA['seconds_after_rat_arrival'])\n",
    "\n",
    "# Standard scaling for continuous predictors\n",
    "for col in ['hours_after_sunset', 'temperature', 'humidity']:\n",
    "    if col in dfA.columns:\n",
    "        dfA[f'{col}_scaled'] = (dfA[col] - dfA[col].mean()) / dfA[col].std()\n",
    "\n",
    "# Interaction terms\n",
    "dfA['interaction_risk_reward'] = dfA['risk'] * dfA['reward']\n",
    "dfA['rat_risk_interaction'] = dfA['rat_present'] * dfA['risk']\n",
    "\n",
    "# Non-linear temporal feature\n",
    "dfA['night_intensity'] = np.exp(-abs(dfA['hours_after_sunset'] - 3))\n",
    "\n",
    "# Define predictors (final refined set)\n",
    "predictorsA = [\n",
    "    'rat_present', 'risk', 'reward', 'hours_after_sunset_scaled',\n",
    "    'log_seconds_after_rat_arrival', 'temperature_scaled', 'humidity_scaled',\n",
    "    'interaction_risk_reward', 'rat_risk_interaction', 'night_intensity'\n",
    "]\n",
    "\n",
    "# Drop missing\n",
    "dfA = dfA.dropna(subset=[response] + predictorsA)\n",
    "\n",
    "# --- Remove multicollinearity ---\n",
    "X = dfA[predictorsA].assign(Intercept=1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"feature\"] = X.columns\n",
    "print(\"VIF Table:\\n\", vif)\n",
    "\n",
    "# Drop variables with VIF > 10 if needed\n",
    "predictorsA = [v for v in predictorsA if v not in vif.loc[vif[\"VIF\"] > 10, \"feature\"].tolist()]\n",
    "\n",
    "# --- Model Formula ---\n",
    "formulaA = response + ' ~ ' + ' + '.join(predictorsA)\n",
    "print('Final Formula A:', formulaA)\n",
    "\n",
    "# --- Fit Model ---\n",
    "modelA = smf.ols(formulaA, data=dfA).fit()\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_true = dfA[response]\n",
    "y_pred = modelA.fittedvalues\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print(modelA.summary())\n",
    "print(f\"R²: {r2:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "# --- Diagnostics ---\n",
    "resid = modelA.resid\n",
    "fitted = modelA.fittedvalues\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(fitted, resid, alpha=0.5)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title('A: Residuals vs Fitted')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT/'A_resid_vs_fitted.png')\n",
    "plt.close()\n",
    "\n",
    "sm.qqplot(resid, line='45')\n",
    "plt.title('A: QQ Plot (Normality)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT/'A_qq.png')\n",
    "plt.close()\n",
    "\n",
    "sns.histplot(resid, kde=True)\n",
    "plt.title(\"A: Residual Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT/'A_resid_hist.png')\n",
    "plt.close()\n",
    "\n",
    "print('Enhanced Investigation A completed and plots saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ca5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial numeric predictors (safe): ['rat_present', 'hours_after_sunset', 'hours_after_sunset_sq', 'risk', 'reward', 'risk_reward_interaction', 'interaction_rat_risk', 'interaction_time_risk', 'reward_to_risk_ratio', 'nocturnal_index', 'month_log1p', 'season_log1p', 'risk_reward_interaction_log1p', 'hours_after_sunset_poly']\n",
      "Dropping constant columns (no variance): ['rat_present']\n",
      "Dropping 'hours_after_sunset' due to high VIF (inf)\n",
      "Dropping 'risk' due to high VIF (inf)\n",
      "Dropping 'reward' due to high VIF (inf)\n",
      "Dropping 'risk_reward_interaction' due to high VIF (inf)\n",
      "Dropping 'hours_after_sunset_poly' due to high VIF (14263.50)\n",
      "Dropping 'month_log1p' due to high VIF (43.05)\n",
      "Dropping 'interaction_time_risk' due to high VIF (10.42)\n",
      "\n",
      "Final predictors after VIF filtering:\n",
      "['hours_after_sunset_sq', 'interaction_rat_risk', 'reward_to_risk_ratio', 'nocturnal_index', 'season_log1p', 'risk_reward_interaction_log1p']\n",
      "                             OLS Regression Results                            \n",
      "===============================================================================\n",
      "Dep. Variable:     bat_landing_to_food   R-squared:                       0.122\n",
      "Model:                             OLS   Adj. R-squared:                  0.116\n",
      "Method:                  Least Squares   F-statistic:                     20.80\n",
      "Date:                 Wed, 15 Oct 2025   Prob (F-statistic):           6.49e-23\n",
      "Time:                         18:44:07   Log-Likelihood:                -4238.3\n",
      "No. Observations:                  907   AIC:                             8491.\n",
      "Df Residuals:                      900   BIC:                             8524.\n",
      "Df Model:                            6                                         \n",
      "Covariance Type:             nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                            11.1243      4.578      2.430      0.015       2.140      20.109\n",
      "hours_after_sunset_sq            -0.0217      0.033     -0.663      0.507      -0.086       0.043\n",
      "interaction_rat_risk             -5.3735      3.368     -1.595      0.111     -11.984       1.237\n",
      "reward_to_risk_ratio            -10.8406      3.347     -3.239      0.001     -17.409      -4.272\n",
      "nocturnal_index                   2.6563      3.837      0.692      0.489      -4.874      10.186\n",
      "season_log1p                      6.4081      3.424      1.871      0.062      -0.313      13.129\n",
      "risk_reward_interaction_log1p    45.1608      4.962      9.102      0.000      35.423      54.898\n",
      "==============================================================================\n",
      "Omnibus:                     1250.759   Durbin-Watson:                   1.953\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           289746.513\n",
      "Skew:                           7.447   Prob(JB):                         0.00\n",
      "Kurtosis:                      89.285   Cond. No.                         350.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Evaluation Metrics:\n",
      "0.13413058890155627\n",
      "Test R²:   0.134\n",
      "Test RMSE: 19.780\n",
      "Test MAE:  10.553\n",
      "Mean CV R²: 0.040 (± 0.129)\n",
      "\n",
      "All outputs saved in: /Users/maze/Documents/FODS/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/6cggrtcs02gfymtzntgnsn700000gn/T/ipykernel_97551/1522836085.py:48: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  st = pd.to_datetime(df['start_time'], errors='coerce')\n",
      "/var/folders/m0/6cggrtcs02gfymtzntgnsn700000gn/T/ipykernel_97551/1522836085.py:49: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  r0 = pd.to_datetime(df['rat_period_start'], errors='coerce')\n",
      "/var/folders/m0/6cggrtcs02gfymtzntgnsn700000gn/T/ipykernel_97551/1522836085.py:50: UserWarning: Parsing dates in %d/%m/%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  r1 = pd.to_datetime(df['rat_period_end'], errors='coerce')\n",
      "/Users/maze/Documents/FODS/env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Users/maze/Documents/FODS/env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Users/maze/Documents/FODS/env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Users/maze/Documents/FODS/env/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "643e5f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets/dataset1.csv\n",
      "Predictor candidates used: ['seconds_after_rat_arrival', 'hours_after_sunset', 'rat_presence_intensity', 'foraging_efficiency', 'risk', 'rat_present']\n",
      "Removing constant or invalid predictor: rat_present\n",
      "Features selected for Yeo-Johnson transform (skew>0.75): ['seconds_after_rat_arrival', 'rat_presence_intensity', 'foraging_efficiency']\n",
      "Dropping rat_presence_intensity (VIF=5.97)\n",
      "Predictors after VIF reduction: ['seconds_after_rat_arrival', 'hours_after_sunset', 'foraging_efficiency', 'risk']\n",
      "Creating interactions among: ['hours_after_sunset', 'foraging_efficiency', 'seconds_after_rat_arrival', 'risk']\n",
      "Dropping foraging_efficiency_x_seconds_after_rat_arrival (p=0.9064)\n",
      "Dropping risk (p=0.5934)\n",
      "Dropping hours_after_sunset (p=0.4216)\n",
      "Dropping seconds_after_rat_arrival (p=0.3859)\n",
      "Dropping hours_after_sunset_x_foraging_efficiency (p=0.3344)\n",
      "Dropping hours_after_sunset_x_risk (p=0.5334)\n",
      "Dropping hours_after_sunset_x_seconds_after_rat_arrival (p=0.2702)\n",
      "Dropping seconds_after_rat_arrival_x_risk (p=0.1156)\n",
      "Final features after backward elimination: ['foraging_efficiency', 'foraging_efficiency_x_risk']\n",
      "\n",
      "Final OLS summary:\n",
      "                             OLS Regression Results                            \n",
      "===============================================================================\n",
      "Dep. Variable:     bat_landing_to_food   R-squared:                       0.099\n",
      "Model:                             OLS   Adj. R-squared:                  0.097\n",
      "Method:                  Least Squares   F-statistic:                     39.74\n",
      "Date:                 Wed, 15 Oct 2025   Prob (F-statistic):           4.24e-17\n",
      "Time:                         19:19:32   Log-Likelihood:                -3212.1\n",
      "No. Observations:                  725   AIC:                             6430.\n",
      "Df Residuals:                      722   BIC:                             6444.\n",
      "Df Model:                            2                                         \n",
      "Covariance Type:             nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                         12.9342      1.097     11.787      0.000      10.780      15.088\n",
      "foraging_efficiency           -5.9239      0.825     -7.184      0.000      -7.543      -4.305\n",
      "foraging_efficiency_x_risk     2.3119      1.149      2.012      0.045       0.056       4.568\n",
      "==============================================================================\n",
      "Omnibus:                      643.899   Durbin-Watson:                   2.021\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            15119.891\n",
      "Skew:                           4.074   Prob(JB):                         0.00\n",
      "Kurtosis:                      23.836   Cond. No.                         2.65\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Final OLS Test metrics: R²=0.0644, RMSE=18.8532, MAE=10.0452\n",
      "Ridge Test metrics: R²=0.0662, RMSE=18.8352, alpha=35.564803062231285\n",
      "Lasso Test metrics: R²=0.0644, RMSE=18.8528, alpha=0.003906939937054617\n",
      "10-fold CV R²: mean=-0.0659, std=0.3893\n",
      "{'final_ols_test_r2': 0.064397266542634, 'final_ols_rmse': 18.853246358220584, 'ridge_test_r2': 0.06619116852324558, 'ridge_rmse': 18.835163307495403, 'lasso_test_r2': 0.06444542054332603, 'lasso_rmse': 18.852761178513674, 'cv_r2_mean': -0.06591862015977397, 'cv_r2_std': 0.3893353609776749, 'num_obs': 907, 'final_predictors': ['foraging_efficiency', 'foraging_efficiency_x_risk']}\n",
      "\n",
      "Outputs saved to folder: /Users/maze/Documents/FODS/outputs\n",
      "Final model performance summary:\n",
      "                                                                   0\n",
      "final_ols_test_r2                                           0.064397\n",
      "final_ols_rmse                                             18.853246\n",
      "ridge_test_r2                                               0.066191\n",
      "ridge_rmse                                                 18.835163\n",
      "lasso_test_r2                                               0.064445\n",
      "lasso_rmse                                                 18.852761\n",
      "cv_r2_mean                                                 -0.065919\n",
      "cv_r2_std                                                   0.389335\n",
      "num_obs                                                          907\n",
      "final_predictors   [foraging_efficiency, foraging_efficiency_x_risk]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Investigation A (Revisited) — Polished & Optimised Linear Regression\n",
    "# - Winsorize outliers\n",
    "# - Gaussian (Yeo-Johnson) transform\n",
    "# - Remove multicollinearity (VIF)\n",
    "# - Backward feature selection (p-value)\n",
    "# - Compare OLS, RidgeCV, LassoCV\n",
    "# - Diagnostics & outputs saved to ./outputs\n",
    "# ================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def safe_load(paths):\n",
    "    for p in paths:\n",
    "        if Path(p).exists():\n",
    "            print(f\"Loading {p}\")\n",
    "            return pd.read_csv(p)\n",
    "    raise FileNotFoundError(f\"None of the candidate paths exist: {paths}\")\n",
    "\n",
    "def winsorize_series(s, lower_q=0.01, upper_q=0.99):\n",
    "    lo = s.quantile(lower_q)\n",
    "    hi = s.quantile(upper_q)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "def calculate_vif(df):\n",
    "    X = sm.add_constant(df)\n",
    "    vif = pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X.values, i + 1) for i in range(len(df.columns))]\n",
    "    }).sort_values(\"VIF\", ascending=False)\n",
    "    return vif\n",
    "\n",
    "def backward_elimination(X, y, p_thresh=0.05, verbose=True):\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_sm).fit()\n",
    "    while True:\n",
    "        pvals = model.pvalues.drop('const')\n",
    "        max_p = pvals.max()\n",
    "        if max_p > p_thresh:\n",
    "            drop_feat = pvals.idxmax()\n",
    "            if verbose:\n",
    "                print(f\"Dropping {drop_feat} (p={max_p:.4f})\")\n",
    "            X = X.drop(columns=[drop_feat])\n",
    "            X_sm = sm.add_constant(X)\n",
    "            model = sm.OLS(y, X_sm).fit()\n",
    "        else:\n",
    "            break\n",
    "    return X, model\n",
    "\n",
    "# -------------------------\n",
    "# Paths & output directory\n",
    "# -------------------------\n",
    "CANDIDATE_PATHS = [\n",
    "    \"dataset1.csv\", \"Dataset1.csv\",\n",
    "    \"datasets/dataset1.csv\", \"/mnt/data/dataset1.csv\", \"data/dataset1.csv\"\n",
    "]\n",
    "OUT = Path(\"outputs\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Load data (df1 is main for Investigation A)\n",
    "# -------------------------\n",
    "df1 = safe_load(CANDIDATE_PATHS)\n",
    "\n",
    "# -------------------------\n",
    "# Basic cleaning & selection\n",
    "# -------------------------\n",
    "# Standardise column names\n",
    "df1.columns = df1.columns.str.strip().str.lower()\n",
    "\n",
    "# Check response candidate(s)\n",
    "# Choose response according to your instruction: continuous variable approximating bats' behaviour\n",
    "response = \"bat_landing_to_food\"\n",
    "if response not in df1.columns:\n",
    "    raise KeyError(f\"Response variable '{response}' not found in dataset columns: {df1.columns.tolist()}\")\n",
    "\n",
    "# Keep only rows with valid response\n",
    "df = df1.copy()\n",
    "df = df.dropna(subset=[response]).reset_index(drop=True)\n",
    "\n",
    "# Candidate predictor list (choose meaningful ones — do NOT include every column blindly)\n",
    "# Start with continuous / ratio variables that are present in the file:\n",
    "candidate_continuous = [\n",
    "    \"seconds_after_rat_arrival\",\n",
    "    \"hours_after_sunset\",\n",
    "    \"rat_minutes\",\n",
    "    \"rat_arrival_number\",\n",
    "    \"bat_landing_number\",\n",
    "    \"food_availability\"\n",
    "]\n",
    "# Add engineered continuous candidates if the raw columns exist\n",
    "engineered_candidates = [\n",
    "    \"rat_presence_intensity\",    # we'll compute if needed\n",
    "    \"foraging_efficiency\"        # we'll compute if needed\n",
    "]\n",
    "\n",
    "# Build list of predictors that actually exist in df (and are numeric-capable)\n",
    "predictors = []\n",
    "for c in candidate_continuous:\n",
    "    if c in df.columns:\n",
    "        predictors.append(c)\n",
    "\n",
    "# create engineered predictors if possible\n",
    "if \"seconds_after_rat_arrival\" in df.columns:\n",
    "    df[\"rat_presence_intensity\"] = 1 / (1 + df[\"seconds_after_rat_arrival\"].astype(float).replace(0, np.nan))\n",
    "    df[\"rat_presence_intensity\"] = df[\"rat_presence_intensity\"].fillna(0)\n",
    "    predictors.append(\"rat_presence_intensity\")\n",
    "\n",
    "# foraging_efficiency: reward / (landing_time + 1) if reward exists\n",
    "if (\"reward\" in df.columns):\n",
    "    df[\"foraging_efficiency\"] = df[\"reward\"].astype(float) / (df[response].astype(float) + 1)\n",
    "    predictors.append(\"foraging_efficiency\")\n",
    "\n",
    "# Include binary/binary-like predictors that are meaningful\n",
    "# e.g., risk, rat presence (if provided)\n",
    "if \"risk\" in df.columns:\n",
    "    # ensure numeric\n",
    "    df[\"risk\"] = pd.to_numeric(df[\"risk\"], errors='coerce').fillna(0)\n",
    "    predictors.append(\"risk\")\n",
    "if \"rat_present\" in df.columns:\n",
    "    # convert to numeric\n",
    "    df[\"rat_present\"] = pd.to_numeric(df[\"rat_present\"], errors='coerce').fillna(0)\n",
    "    predictors.append(\"rat_present\")\n",
    "elif \"seconds_after_rat_arrival\" in df.columns:\n",
    "    # derive rat_present from seconds_after_rat_arrival >= 0\n",
    "    df[\"rat_present\"] = (~df[\"seconds_after_rat_arrival\"].isna()) & (df[\"seconds_after_rat_arrival\"] >= 0)\n",
    "    df[\"rat_present\"] = df[\"rat_present\"].astype(int)\n",
    "    if \"rat_present\" not in predictors:\n",
    "        predictors.append(\"rat_present\")\n",
    "\n",
    "# Remove duplicates and ensure predictors exist\n",
    "predictors = [p for p in predictors if p in df.columns]\n",
    "print(\"Predictor candidates used:\", predictors)\n",
    "\n",
    "# Drop any predictors that are constant or non-numeric after coercion\n",
    "clean_preds = []\n",
    "for p in predictors:\n",
    "    df[p] = pd.to_numeric(df[p], errors=\"coerce\")\n",
    "    if df[p].nunique(dropna=True) > 1:\n",
    "        clean_preds.append(p)\n",
    "    else:\n",
    "        print(f\"Removing constant or invalid predictor: {p}\")\n",
    "predictors = clean_preds\n",
    "\n",
    "# Final dataset for modeling\n",
    "model_df = df[[response] + predictors].copy()\n",
    "# Fill numeric NaNs by median (safer than mean)\n",
    "for col in model_df.columns:\n",
    "    if model_df[col].isna().any():\n",
    "        model_df[col] = model_df[col].fillna(model_df[col].median())\n",
    "\n",
    "# -------------------------\n",
    "# Winsorize outliers at 1% / 99%\n",
    "# -------------------------\n",
    "for col in model_df.select_dtypes(include=[np.number]).columns:\n",
    "    model_df[col] = winsorize_series(model_df[col], 0.01, 0.99)\n",
    "\n",
    "# -------------------------\n",
    "# Log / Yeo-Johnson (Gaussianize) transform for skewed predictors\n",
    "# -------------------------\n",
    "# Use Yeo-Johnson (works with zero/negative)\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize=False)\n",
    "X_raw = model_df[predictors].astype(float)\n",
    "# detect skewness and transform only if skew > 0.75 (heuristic)\n",
    "skews = X_raw.skew().abs()\n",
    "to_transform = skews[skews > 0.75].index.tolist()\n",
    "print(\"Features selected for Yeo-Johnson transform (skew>0.75):\", to_transform)\n",
    "if to_transform:\n",
    "    X_trans = X_raw.copy()\n",
    "    X_trans[to_transform] = pt.fit_transform(X_trans[to_transform])\n",
    "else:\n",
    "    X_trans = X_raw.copy()\n",
    "\n",
    "# -------------------------\n",
    "# Remove multicollinearity via iterative VIF removal (threshold 5.0)\n",
    "# -------------------------\n",
    "X_vif = X_trans.copy()\n",
    "def iterative_vif_removal(X_df, thresh=5.0):\n",
    "    X_work = X_df.copy()\n",
    "    while True:\n",
    "        vif = calculate_vif(X_work)\n",
    "        max_vif = vif[\"VIF\"].max()\n",
    "        if max_vif > thresh:\n",
    "            drop_feat = vif.sort_values(\"VIF\", ascending=False)[\"feature\"].iloc[0]\n",
    "            print(f\"Dropping {drop_feat} (VIF={max_vif:.2f})\")\n",
    "            X_work = X_work.drop(columns=[drop_feat])\n",
    "        else:\n",
    "            break\n",
    "    return X_work\n",
    "\n",
    "X_nmv = iterative_vif_removal(X_vif, thresh=5.0)\n",
    "print(\"Predictors after VIF reduction:\", list(X_nmv.columns))\n",
    "\n",
    "# -------------------------\n",
    "# Standardize predictors\n",
    "# -------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_nmv), columns=X_nmv.columns, index=X_nmv.index)\n",
    "\n",
    "# -------------------------\n",
    "# Optionally add polynomial / interaction terms (only a few to avoid explosion)\n",
    "# We'll add pairwise interactions for a small subset (top 4 features by variance)\n",
    "# -------------------------\n",
    "# choose up to 4 most variable features\n",
    "vars_by_var = X_scaled.var().sort_values(ascending=False).index.tolist()\n",
    "interaction_feats = vars_by_var[:4]\n",
    "print(\"Creating interactions among:\", interaction_feats)\n",
    "for i in range(len(interaction_feats)):\n",
    "    for j in range(i+1, len(interaction_feats)):\n",
    "        a = interaction_feats[i]\n",
    "        b = interaction_feats[j]\n",
    "        name = f\"{a}_x_{b}\"\n",
    "        X_scaled[name] = X_scaled[a] * X_scaled[b]\n",
    "\n",
    "# -------------------------\n",
    "# Backward feature selection by p-value on OLS (start with current X_scaled)\n",
    "# -------------------------\n",
    "X_for_model = X_scaled.copy()\n",
    "y_final = model_df[response].astype(float)\n",
    "\n",
    "X_selected, ols_full = backward_elimination(X_for_model, y_final, p_thresh=0.05, verbose=True)\n",
    "print(\"Final features after backward elimination:\", list(X_selected.columns))\n",
    "\n",
    "# -------------------------\n",
    "# Train/Test split\n",
    "# -------------------------\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_selected, y_final, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# Fit OLS (final) and evaluate\n",
    "# -------------------------\n",
    "X_tr_sm = sm.add_constant(X_tr)\n",
    "final_ols = sm.OLS(y_tr, X_tr_sm).fit()\n",
    "print(\"\\nFinal OLS summary:\")\n",
    "print(final_ols.summary())\n",
    "\n",
    "X_te_sm = sm.add_constant(X_te)\n",
    "y_pred_ols = final_ols.predict(X_te_sm)\n",
    "\n",
    "r2_ols = r2_score(y_te, y_pred_ols)\n",
    "rmse_ols = np.sqrt(mean_squared_error(y_te, y_pred_ols))\n",
    "mae_ols = mean_absolute_error(y_te, y_pred_ols)\n",
    "print(f\"\\nFinal OLS Test metrics: R²={r2_ols:.4f}, RMSE={rmse_ols:.4f}, MAE={mae_ols:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Regularized models (RidgeCV, LassoCV) for comparison & stability\n",
    "# -------------------------\n",
    "ridge = RidgeCV(alphas=np.logspace(-4, 4, 50), cv=5).fit(X_tr, y_tr)\n",
    "y_pred_ridge = ridge.predict(X_te)\n",
    "r2_ridge = r2_score(y_te, y_pred_ridge); rmse_ridge = np.sqrt(mean_squared_error(y_te, y_pred_ridge))\n",
    "\n",
    "lasso = LassoCV(alphas=np.logspace(-4, 2, 50), cv=5, max_iter=10000).fit(X_tr, y_tr)\n",
    "y_pred_lasso = lasso.predict(X_te)\n",
    "r2_lasso = r2_score(y_te, y_pred_lasso); rmse_lasso = np.sqrt(mean_squared_error(y_te, y_pred_lasso))\n",
    "\n",
    "print(f\"Ridge Test metrics: R²={r2_ridge:.4f}, RMSE={rmse_ridge:.4f}, alpha={ridge.alpha_}\")\n",
    "print(f\"Lasso Test metrics: R²={r2_lasso:.4f}, RMSE={rmse_lasso:.4f}, alpha={lasso.alpha_}\")\n",
    "\n",
    "# -------------------------\n",
    "# Cross-validation for final OLS (10-fold)\n",
    "# -------------------------\n",
    "cv_scores = cross_val_score(LinearRegression(), X_selected, y_final, cv=10, scoring=\"r2\")\n",
    "print(f\"10-fold CV R²: mean={cv_scores.mean():.4f}, std={cv_scores.std():.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Diagnostic plots: residuals vs fitted and QQ and Actual vs Predicted\n",
    "# -------------------------\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# OLS diagnostics on test set\n",
    "residuals = y_te - y_pred_ols\n",
    "fitted = y_pred_ols\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.scatterplot(x=fitted, y=residuals, alpha=0.6)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Fitted (Final OLS)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"A_resid_vs_fitted_two.png\")\n",
    "plt.close()\n",
    "\n",
    "# QQ plot\n",
    "plt.figure(figsize=(6,6))\n",
    "sm.qqplot(residuals, line=\"45\", fit=True)\n",
    "plt.title(\"QQ plot (Final OLS residuals)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"A_qq_two_final.png\")\n",
    "plt.close()\n",
    "\n",
    "# Actual vs predicted\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=y_te, y=y_pred_ols, alpha=0.6)\n",
    "plt.plot([y_te.min(), y_te.max()], [y_te.min(), y_te.max()], color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted (Final OLS)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"A_actual_vs_predicted_final.png\")\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs & summary\n",
    "# -------------------------\n",
    "model_summary = final_ols.summary().as_text()\n",
    "with open(OUT / \"InvestigationA_final_OLS_summary.txt\", \"w\") as f:\n",
    "    f.write(model_summary)\n",
    "\n",
    "results = {\n",
    "    \"final_ols_test_r2\": float(r2_ols),\n",
    "    \"final_ols_rmse\": float(rmse_ols),\n",
    "    \"ridge_test_r2\": float(r2_ridge),\n",
    "    \"ridge_rmse\": float(rmse_ridge),\n",
    "    \"lasso_test_r2\": float(r2_lasso),\n",
    "    \"lasso_rmse\": float(rmse_lasso),\n",
    "    \"cv_r2_mean\": float(cv_scores.mean()),\n",
    "    \"cv_r2_std\": float(cv_scores.std()),\n",
    "    \"num_obs\": int(model_df.shape[0]),\n",
    "    \"final_predictors\": list(X_selected.columns)\n",
    "}\n",
    "\n",
    "print(results)\n",
    "\n",
    "pd.DataFrame([results]).to_csv(OUT / \"InvestigationA_results_summary.csv\", index=False)\n",
    "model_df.to_csv(OUT / \"InvestigationA_model_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nOutputs saved to folder:\", OUT.resolve())\n",
    "print(\"Final model performance summary:\")\n",
    "print(pd.DataFrame([results]).T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
